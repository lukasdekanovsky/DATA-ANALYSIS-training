INTRODUCTION PySpark
-   PySpark is interface for Apache Spark in python 
-   Used for large data processing and machine learning
-   For huge amount of data (128 GB)

INTRODUCTION Apache Spark
-   Apache Spark is a multi-language engine for executing data science and ML on single node machines or clusetres
-   Using for larg scale data processing
-   Its really fast

1) WORKFLOW BASIC 
    - import module             - import pyspark 
    - start a Spark session     - from pyspark.sql import SparkSession
                                - spark = SparkSession.builder.appName("test").getOrCreate()
                                - spark
    - read dataset  opt. 1      - dataframe = spark.read.csv("path", header=True, inferSchema=True, spe=";")
    - read dataset  opt. 2      - dataframe = spark.read.option("header", "true").csv("path")
    - show dataset              - dataframe.show()
    - basic investigation       - dataframe.head() 
                                - dataframe.printSchema()   -> info about datatypes (int, str... in columns)
                                - dataframe.select("column").show()

2) SELECTING COLUMNS AND INDEXING
    - selecting the column heads            - dataframe.columns
    - selecting just specific columns       - dataframe.select(["column_name", "column2_name"])

3) CHECKING DATATYPES    
    - checking a datatypes                  - dataframe.dtypes
                                            - dataframe.describe()

4) ADDING COLUMNS              
    - add column                            - dataframe.withColumn("new_column_name", some mathematical operation)
    - drop column                           - dataframe.drop("column_name")

5) RENAME COLUMN 
    - rename                                - dataframe.withColumnRenamed("old_name", "new_name")

6) HANDLING MISSING VALUES
    - dropping columns                      - dataframe.drop("column_name")
    - replacing null values                 - dataframe.na.drop/fill/replace
                                            - dataframe.na.drop(how="any", tresh=None, subset=None)  -> it will delete all rows which contains null values
                                                                                                     -> how = "any" or "all" --- all means it will delete only rows with all nulls 
                                                                                                     -> tresh = 2 -- at least 2 null values needs to be presented for row delete
                                                                                                     -> subset = ["column_name"] --- delete rows where named column contains null 
                                            - dataframe.na.fill("Missing value", ["col_1", "col_2"]) 
                                            - dataframe.na.replace()
    - using imputer (mean, avarage.. replace)
                                            - from pyspark.ml.feature import Imputer 
                                            - imputer = Imputer(
                                                inpuCols=["age", "experience", "salary"],
                                                outputCols=["{}_imputed".format(c) for c in ["age", "experience", "salary"]]
                                            ).setStrategy("mean")
                                            - imputer.fit(dataframe).transform(dataframe).show()